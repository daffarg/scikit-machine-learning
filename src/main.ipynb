{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m      3\u001b[0m sys\u001b[39m.\u001b[39mmodules[\u001b[39m'\u001b[39m\u001b[39msklearn.externals.six\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m six\n\u001b[1;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score, precision_score, f1_score, recall_score, confusion_matrix\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_breast_cancer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "\n",
    "from id3 import Id3Estimator\n",
    "import id3.export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569\n",
      "114\n",
      "455\n",
      "test 0.20035149384885764\n",
      "train 0.7996485061511424\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    TASK 1\n",
    "    Membaca dataset breast cancer\n",
    "'''\n",
    "\n",
    "# load dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# Seperate feature and target\n",
    "feat = df.iloc[:, :-1]\n",
    "target = df['target']\n",
    "\n",
    "# Check value for target function and feature\n",
    "# print(\"x_bc\", x_bc[:5])\n",
    "# print(\"y_bc\", y_bc.unique())\n",
    "\n",
    "\n",
    "# Separate datasets into 80% training data and 20% test data\n",
    "data_train, data_test, target_train, target_test = train_test_split(feat, target, test_size=0.2, random_state=420)\n",
    "\n",
    "'''\n",
    "    OUTPUT:\n",
    "    data_train: data yang digunakan sebagai data training\n",
    "    data_test: data yang digunakan sebagai data testing\n",
    "    target_train: target atau label yang sesuai dengan data training\n",
    "    target_test: target atau label yang sesuai dengan data testing\n",
    "\n",
    "    INPUT:\n",
    "    feat: atribut dari dataset yang akan dibagi menjadi data training dan data testing\n",
    "    target: target atau label dari dataset yang akan dibagi menjadi data training dan data testing\n",
    "    test_size=0.2: ukuran data testing, di sini diatur sebesar 20% dari seluruh dataset\n",
    "    random_state=42: seed atau biji acak yang digunakan untuk memastikan hasil pengacakan selalu sama setiap kali kode dijalankan, \n",
    "    sehingga hasil yang diperoleh dapat direproduksi.\n",
    "'''\n",
    "\n",
    "#Checking the result\n",
    "print(len(data.data)) \n",
    "print(len(data_test))\n",
    "print(len(data_train))\n",
    "print('test',len(data_test)/len(data.data))\n",
    "print('train',len(data_train)/len(data.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Function for a train and prediction\n",
    "->def get_predictions(model,data_train, target_train, data_test)\n",
    "    param for input :\n",
    "    1. model -> learning model\n",
    "    2. data_train -> data training\n",
    "    3. target_train -> target training \n",
    "    4. data_test  -> data test\n",
    "    This function will return the prediction of    \n",
    "-> def get_score(model, data_train, target_train, data_test, target_test)\n",
    "    param for input :\n",
    "    1. model -> learning model\n",
    "    2. data_train -> data training\n",
    "    3. target_train -> target training\n",
    "    4. data_test -> data test\n",
    "    5. target_test -> target training\n",
    "    This function will return the value of all the accuracy score and f1_score of the entire prediction \n",
    "    as dictionary\n",
    "'''\n",
    "\n",
    "def get_prediction(model, model_name, data_train, target_train, data_test):\n",
    "    model  = model.fit(data_train, target_train)\n",
    "    save_to_pickle(model, model_name)\n",
    "    return model.predict(data_test)\n",
    "\n",
    "def save_to_pickle(model,filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model,file)\n",
    "        \n",
    "def get_score(model, model_name, data_train, target_train, data_test, target_test):\n",
    "    prediction = get_prediction(model, model_name, data_train, target_train, data_test)\n",
    "    return {\n",
    "        \"accuracy_score\": accuracy_score(target_test, prediction),\n",
    "        \"precision_score\": precision_score(target_test, prediction),\n",
    "        \"recall_score\": recall_score(target_test, prediction),\n",
    "        \"f1_score\": f1_score(target_test, prediction, average='micro'),\n",
    "        \"confusion_matrix\": confusion_matrix(target_test, prediction)\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_score': 0.9736842105263158, 'precision_score': 0.9710144927536232, 'recall_score': 0.9852941176470589, 'f1_score': 0.9736842105263158, 'confusion_matrix': array([[44,  2],\n",
      "       [ 1, 67]], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    DECISION TREE CLASSIFIER\n",
    "\n",
    "    Create learning model with decision tree classfier\n",
    "    Train the data and get score\n",
    "\n",
    "    Param:\n",
    "    1. criterion = entropy -> use Information Gain measurement in selecting \n",
    "    the best feature for splitting\n",
    "    2. max_features = auto ->  select the best feature considering the square root \n",
    "    of the number of features\n",
    "    3. random state -> set seed for the algorithm's randomization\n",
    "'''\n",
    "\n",
    "dtl = DecisionTreeClassifier(criterion=\"entropy\", max_features=\"auto\", random_state=33)\n",
    "\n",
    "dt_score = get_score(dtl, \"DTL.pkl\", data_train, target_train, data_test, target_test)\n",
    "print(dt_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_score': 0.9210526315789473, 'precision_score': 0.927536231884058, 'recall_score': 0.9411764705882353, 'f1_score': 0.9210526315789473, 'confusion_matrix': array([[41,  5],\n",
      "       [ 4, 64]], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    ID3\n",
    "\n",
    "    Create learning model with ID3\n",
    "    Train the data and get score\n",
    "\n",
    "    Param:\n",
    "    1. prune = True -> the resulting decision tree will be pruned to prevent overfitting\n",
    "    2. gain_ratio = True -> use the gain ratio metric to measure the information \n",
    "    value of each feature in splitting the dataset\n",
    "'''\n",
    "\n",
    "id3 = Id3Estimator(prune=True, gain_ratio=True)\n",
    "id3_score = get_score(id3, \"ID3.pkl\", data_train, target_train, data_test, target_test)\n",
    "\n",
    "print(id3_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_score': 0.868421052631579, 'precision_score': 0.8271604938271605, 'recall_score': 0.9852941176470589, 'f1_score': 0.868421052631579, 'confusion_matrix': array([[32, 14],\n",
      "       [ 1, 67]], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    K-Means\n",
    "\n",
    "    Create learning model with K-Means\n",
    "    Train the data and get score\n",
    "\n",
    "    Param:\n",
    "    1. n_cluesters = 2 -> number of clusters = 2\n",
    "    2. max_iter = 10000 -> maximum number of iterations the K-Means algorithm = 10000\n",
    "    3. random_state = 13 ->  seed for the random number generator used by the K-Means algorithm = 13\n",
    "'''\n",
    "\n",
    "# setting the random_state will make the result remain the same for every run\n",
    "kmeans = KMeans(n_clusters=2, max_iter=10000, random_state=13) \n",
    "kmeans_score = get_score(kmeans, \"K-MEANS.pkl\", data_train, target_train, data_test, target_test)\n",
    "\n",
    "print(kmeans_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_score': 0.9473684210526315, 'precision_score': 0.9558823529411765, 'recall_score': 0.9558823529411765, 'f1_score': 0.9473684210526315, 'confusion_matrix': array([[43,  3],\n",
      "       [ 3, 65]], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Logistic Regression\n",
    "\n",
    "    Create learning model with K-Means\n",
    "    Train the data and get score\n",
    "    Param :\n",
    "    max_iter = 10000 -> maximum number of iterations the Logistic Regression algorithm \n",
    "\n",
    "'''\n",
    "\n",
    "logres = LogisticRegression(max_iter=10000)\n",
    "logres_score = get_score(logres,'LOGRES.pkl', data_train, target_train, data_test, target_test)\n",
    "\n",
    "print(logres_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_score': 0.9649122807017544, 'precision_score': 0.9848484848484849, 'recall_score': 0.9558823529411765, 'f1_score': 0.9649122807017544, 'confusion_matrix': array([[45,  1],\n",
      "       [ 3, 65]], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Multilayer Perceptron (MLP)\n",
    "\n",
    "    Create learning model with K-Means\n",
    "    Train the data and get score\n",
    "\n",
    "    Param:\n",
    "    1. n_cluesters = 50000 -> maximum number of iterations for the solver to converge = 50000\n",
    "    2. solver = lbfgs -> optimization solver algorithm to be used = lbfgs\n",
    "    The 'lbfgs' solver is used to optimize the weights and bias parameters of the network\n",
    "\n",
    "     \"Limited-memory Broyden-Fletcher-Goldfarb-Shanno\" \n",
    "     and is a quasi-Newton method to approximate the Newton-Raphson algorithm.\n",
    "\n",
    "'''\n",
    "\n",
    "mlp = MLPClassifier(max_iter=50000, solver=\"lbfgs\")\n",
    "mlp_score = get_score(mlp,'MLP.pkl',data_train, target_train, data_test, target_test)\n",
    "\n",
    "print(mlp_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_score': 0.9649122807017544, 'precision_score': 0.9705882352941176, 'recall_score': 0.9705882352941176, 'f1_score': 0.9649122807017544, 'confusion_matrix': array([[44,  2],\n",
      "       [ 2, 66]], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Support Vector Machine (SVM)\n",
    "\n",
    "    Create learning model with SVM\n",
    "    Train the data and get score\n",
    "\n",
    "    Param:\n",
    "    1. kernel = linear -> linear decision boundary will be used to separate the data into classes\n",
    "'''\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc_score = get_score(svc,'SVC.pkl', data_train, target_train, data_test, target_test)\n",
    "\n",
    "print(svc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.00401497, 0.00399876, 0.00208211, 0.00400805, 0.00195789,\n",
      "       0.0030992 , 0.00288773, 0.00209188, 0.00308895, 0.00300312]), 'score_time': array([0.00199986, 0.00200272, 0.00153089, 0.00107574, 0.00196886,\n",
      "       0.00100112, 0.00199842, 0.00191355, 0.00100803, 0.00089836]), 'test_accuracy': array([0.98245614, 0.92982456, 0.9122807 , 0.89473684, 0.94736842,\n",
      "       0.96491228, 0.96491228, 0.92982456, 0.94736842, 0.91071429]), 'test_f1': array([0.98550725, 0.94444444, 0.92957746, 0.91666667, 0.96      ,\n",
      "       0.97142857, 0.97297297, 0.94444444, 0.95652174, 0.92753623])}\n"
     ]
    }
   ],
   "source": [
    "cv_results = cross_validate(dtl, feat, target, cv=10, scoring=('accuracy', 'f1'))\n",
    "print(\"Accuracy: \", cv_results['test_accuracy'].mean())\n",
    "print(\"Precision: \", cv_results['test_precision'].mean())\n",
    "print(\"Recall: \", cv_results['test_recall'].mean())\n",
    "print(\"F1-Score: \", cv_results['test_f1'].mean())\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Analisis\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fd7379c5d0f5f01751ef565bb161eac66a46046f496d2954f8d767960123535"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
